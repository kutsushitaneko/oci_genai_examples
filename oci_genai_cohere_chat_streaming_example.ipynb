{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Generative AI Service ストリーミングチャットサンプルノートブック（Cohere Command R/R+/A）\n",
    "関連ドキュメント： Accessing The Code (https://docs.oracle.com/en-us/iaas/Content/generative-ai/get-code.htm#get-code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリインポート\n",
    "- oci : OCI SDK（ソフトウェア開発キット : https://docs.oracle.com/ja-jp/iaas/Content/API/Concepts/sdks.htm ）\n",
    "- os ：オペレーティングシステムへのアクセスを提供するライブラリ（標準ライブラリ : https://docs.python.org/ja/3/library/os.html ）\n",
    "- time : 時刻データへのアクセスと変換（標準ライブラリ : https://docs.python.org/ja/3/library/time.html ）\n",
    "- dotenv : 環境変数を管理するためのライブラリ（python-dotenv : https://pypi.org/project/python-dotenv/ ）\n",
    "- json ：JSON エンコーダーとデコーダー（標準ライブラリ : https://docs.python.org/ja/3/library/json.html ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境変数設定\n",
    "事前準備として \".env\" ファイルに OCI のコンパートメントID と Cohere Command R/R+ のモデルID を記載しておきます。\n",
    "- OCI_COMPARTMENT_ID=XXXXXXXXXX の書式でコンパートメントIDを記載\n",
    "- OCI_GENAI_MODEL_ID=xxxxxxxxxx の書式でモデルID を記載（例：cohere.command-a-03-2025）\n",
    "\n",
    "利用可能なモデルは、下記公式ドキュメントで確認できます。\n",
    "日本語：[Oracle Cloud Infrastructureドキュメント >> 生成AI >> 生成AIでの事前トレーニング済基礎モデル](https://docs.oracle.com/ja-jp/iaas/Content/generative-ai/pretrained-models.htm#pretrained-models)\n",
    "英語：[Oracle Cloud Infrastructure Documentation >> Generative AI >> Pretrained Foundational Models in Generative AI](https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm#pretrained-models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_= load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCI 認証設定\n",
    "- CONFIG_PROFILE：構成ファイルに定義されたプロファイル名\n",
    "- config : SDK and Tool Configuration（認証に関する構成情報を定義するディクショナリー）[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/configuration.html)\n",
    "- region : 利用するOCI Generative AI サービスのリージョンを Region Identifier で指定します。\n",
    "\n",
    "参考ドキュメント\n",
    "- [SDKおよびCLIの構成ファイル](https://docs.oracle.com/ja-jp/iaas/Content/API/Concepts/sdkconfig.htm)\n",
    "- [Configuration](https://docs.oracle.com/en-us/iaas/tools/python/latest/configuration.html)\n",
    "- 利用可能なリージョンは、下記公式ドキュメントで確認できます。\n",
    "    - 日本語：[Oracle Cloud Infrastructureドキュメント >> 生成AI >> 生成AIでの事前トレーニング済基礎モデル](https://docs.oracle.com/ja-jp/iaas/Content/generative-ai/pretrained-models.htm#pretrained-models)\n",
    "    - 英語：[Oracle Cloud Infrastructure Documentation >> Generative AI >> Pretrained Foundational Models in Generative AI](https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm#pretrained-models)\n",
    "- 各リージョンの Region Identifier は下記公式ドキュメントで確認できます。\n",
    "    - 日本語：[リージョンおよび可用性ドメイン](https://docs.oracle.com/ja-jp/iaas/Content/General/Concepts/regions.htm)\n",
    "        日本語ページはリージョン識別子（Region Identifier）の一部が日本語に翻訳されてしまって間違っています。英語ページも併せてご確認ください。\n",
    "    - 英語：[Regions and Availability Domains](https://docs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PROFILE = \"DEFAULT\" # 構成ファイルに合わせて変更してください。\n",
    "config = oci.config.from_file(file_location='~/.oci/config', profile_name=CONFIG_PROFILE)\n",
    "config[\"region\"] = \"us-chicago-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Generative AI Service （生成AIサービス）設定\n",
    "- compartment_id ： OCIコンパートメントID\n",
    "- （オプション）endpoint ： Generative AI Service （生成AIサービス）のサービスエンドポイントURL（カスタム・モデル、もしくは、ホスティング専用AIクラスタを使用する際に指定します。 このノートブックでは使用しません）\n",
    "- model_id ： 推論に使用する基盤モデルのID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id:cohere.command-a-03-2025\n"
     ]
    }
   ],
   "source": [
    "compartment_id = os.getenv(\"OCI_COMPARTMENT_ID\") \n",
    "#endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" # カスタム・モデル、もしくは、ホスティング専用AIクラスタを使用する際に指定します。生成AIコンソールのエンドポイントで確認できます。\n",
    "model_id = os.getenv(\"OCI_GENAI_MODEL_ID\")\n",
    "print(f\"model_id:{model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative AI Service （生成AIサービス）の推論クライアントの生成\n",
    "- GenerativeAiInferenceClient：Generative AI Service （生成AIサービス）の推論クライアントクラス[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/client/oci.generative_ai_inference.GenerativeAiInferenceClient.html#oci.generative_ai_inference.GenerativeAiInferenceClient)\n",
    "  - コンストラクタ：生成AIサービスの基盤モデルで推論を行うためのサービスクライアントを生成\n",
    "      - config : SDK and Tool Configuration（ディクショナリー）[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/configuration.html)\n",
    "      - service_endpoint : （オプション）サービスエンドポイント（カスタム・モデル、もしくは、ホスティング専用AIクラスタを使用する際に指定）\n",
    "  - メソッド\n",
    "      - chat ：チャット応答を生成するメソッド（チャット応答の生成で呼び出す）\n",
    "      - embed_text ：入力データの埋め込み（エンベディング、ベクトルデータ）を生成するメソッド（このノートブックでは使用しない）\n",
    "      - generate_text ：ユーザープロンプトに基づいてテキスト応答を生成するメソッド（このノートブックでは使用しない）\n",
    "      - summarize_text ：入力テキストを要約するメソッド（このノートブックでは使用しない）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "#generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャットリクエストを定義\n",
    "**ストリーム形式の場合、 is_stream を \"True\" に設定します。**\n",
    "\n",
    "CohereChatRequest : Cohere の基盤モデルへのチャットリクエストを定義したクラス。 BaseChatRequest クラスの派生クラス。[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereChatRequest.html#oci.generative_ai_inference.models.CohereChatRequest)\n",
    "- prompt : プロンプト\n",
    "- **is_stream : 部分的な進行状況をストリームバックするかどうか。True に設定されている場合、利用可能となったトークンから順次 server-sent イベントとして送信される。**[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereChatRequest.html#oci.generative_ai_inference.models.CohereChatRequest.is_stream)\n",
    "- max_tokens : 出力トークンの最大数\n",
    "- temperature : 生成される出力のランダム性を設定する数値。値が低いほどランダム性が小さい。\n",
    "- frequency_penalty : 生成されたトークンの反復を減らすために、生成されたテキスト内の頻度に基づいて新しいトークンにペナルティを与えます。大きな数値は、モデルに新しいトークンを使用するように促し、小さな数値はトークンを繰り返すように促す。無効にするには、0に設定する。\n",
    "- top_p : トークン群のうちその生成確率を確率の高いものから順に累計した合計が top_p となるまでのトークン群のみを候補として生成を行う。出現頻度の低いトークンを生成させたくない場合は小さな値を設定する。\n",
    "- top_k ：トークン群のついその生成確率が高いものから k 個のトークン群のみを候補して生成を行う。\n",
    "- is_echo ：Trueの場合にモデルに送信された完全なプロンプトを返します。\n",
    "- chat_history ： ユーザーとモデル間の過去のメッセージのリスト。以下の3種類のメッセージを含むリストを設定します。\n",
    "  - CohereSystemMessage ： システムロールのメッセージを定義したクラス。 CohereMessage クラスの派生クラス。[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereSystemMessage.html)\n",
    "  - CohereUserMessage ： ユーザーロールのメッセージを定義したクラス。 CohereMessage クラスの派生クラス。[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereUserMessage.html)\n",
    "  - CohereChatBotMessage ： チャットボットロールのメッセージを定義したクラス。 CohereMessage クラスの派生クラス。[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereChatBotMessage.html)\n",
    "- documents ： ユーザーのリクエストに対して根拠のある応答を生成するためにモデルが参照できる関連文書のリスト。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "chat_request.message = \"オラクルのリレーショナルデータベースについて教えてください。\"\n",
    "chat_request.max_tokens = 500\n",
    "chat_request.is_stream = True\n",
    "chat_request.temperature = 0.75\n",
    "chat_request.top_p = 0.7\n",
    "chat_request.top_k = 0 # Only support topK within [0, 500]\n",
    "chat_request.frequency_penalty = 1.0\n",
    "chat_request.is_echo = True\n",
    "\n",
    "previous_chat_system_message = oci.generative_ai_inference.models.CohereSystemMessage(role=\"SYSTEM\", message=\"あなたはIT業界に精通した優秀なテクニカルライターです。\")\n",
    "previous_chat_user_message = oci.generative_ai_inference.models.CohereUserMessage(role=\"USER\", message=\"オラクルとはどんな会社ですか？\")\n",
    "previous_chat_chatbot_message = oci.generative_ai_inference.models.CohereChatBotMessage(role=\"CHATBOT\", message=\"Oracleは、エンタープライズIT市場における最大手のベンダーの一つであり、その主力製品の略称でもあります。このデータベースソフトウェアは、多くの企業のITシステムで利用されています。\")\n",
    "chat_request.chat_history = [previous_chat_system_message, previous_chat_user_message, previous_chat_chatbot_message]\n",
    "chat_request.documents = [\n",
    "    {\n",
    "        \"title\": \"Oracle\",\n",
    "        \"snippet\": \"オラクルのデータベース・サービスおよび製品は、世界をリードするコンバージド・マルチモデル・データベース管理システムであるOracle Databaseをはじめ、インメモリ、NoSQLMySQLデータベースなど、お客様に最適なコストとパフォーマンスを提供しています。Oracle Autonomous Databaseは、Oracle Cloud@CustomerまたはOracle Cloud Infrastructureによって、提供されており、お客様は、リレーショナル・データベース環境を簡素化し、管理ワークロードを削減できます。\",\n",
    "        \"website\": \"https://www.oracle.com/jp/database/\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Oracle Database\",\n",
    "        \"snippet\": \"Oracle Database（オラクル データベース）とは、米国オラクル (Oracle) が開発・販売している、関係データベース管理システム ( 英語: Relational database management system、略称：RDBMS ) のことである。Oracle Databaseは世界初の商用RDBMSであり、メインフレームからパーソナルコンピュータまで、幅広いプラットフォームをサポートしている。\",\n",
    "        \"website\": \"https://ja.wikipedia.org/wiki/Oracle_Database\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Oracle Database 23ai\",\n",
    "        \"snippet\": \"Oracle Database 23aiでは、新しい世代のAIモデルを活用してベクトルを生成・格納できる強力な新技術、AI ベクトル検索を導入しました。このベクトルとは、埋込みと呼ばれることもあり、ドキュメント、イメージ、ビデオ、サウンドなどを多次元表現したものです。こうしたオブジェクトをベクトルとしてエンコードすることで、数学計算を使用してそれらの間の類似性を検索できます。Oracle Database23aiのソリューションの真のパワーは、SQLを使用して簡単にこれらの類似性検索とビジネス・データの検索を組み合せることができることです。SQLの基本的な知識があれば、誰でも類似性やその他の検索条件を組み合せた強力なステートメントを作成できます。問合せを組み合わせることで、LLMに追加のコンテキストを提供してＬＬＭの知識を拡張し、顧客や組織の質問に対してより正確で関連性の高い回答を可能にします。この実現に向け、新しいデータ型、新しいベクトル索引および拡張機能をSQL言語に追加したことで、ベクトルを問い合わせを、既存のビジネス・データと組合せてOracle Database 23aiの高度な分析機能を活かし、非常に簡単に実行できます。\",\n",
    "        \"website\": \"https://blogs.oracle.com/oracle4engineer/post/ja-oracle-23ai-now-generally-available\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャット応答生成リクエストの詳細を定義\n",
    "ChatDetails ：チャット応答生成リクエストの詳細を定義したクラス（HTTP リクエストの body となる） [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.ChatDetails.html)\n",
    "  - generative_ai_inference_client の chat メソッドの引数となる\n",
    "  - compartment_id : OCIコンパートメントID\n",
    "  - chat_request : チャットリクエストを定義した CohereChatRequest クラスのインスタンス\n",
    "  - serving_mode : モデルのサービングモード\n",
    "    - OnDemandServingMode : カスタム・モデル、ホスティング専用AIクラスタを使用しない場合\n",
    "    - DedicatedServingMode : カスタム・モデル、もしくは、ホスティング専用AIクラスタを使用する場合\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "\n",
    "chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id)\n",
    "chat_detail.compartment_id = compartment_id\n",
    "chat_detail.chat_request = chat_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャット応答の生成\n",
    "chat : ユーザープロンプトに基づいてチャット応答を生成するメソッド  [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/client/oci.generative_ai_inference.GenerativeAiInferenceClient.html#oci.generative_ai_inference.GenerativeAiInferenceClient.chat)\n",
    "- oci.generative_ai_inference.GenerativeAiInferenceClient クラスのメソッド  [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/client/oci.generative_ai_inference.GenerativeAiInferenceClient.html#oci.generative_ai_inference.GenerativeAiInferenceClient)\n",
    "- 引数 : チャット応答生成リクエストの詳細クラス ChatDetails のインスタンス  [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.ChatDetails.html#oci.generative_ai_inference.models.ChatDetails)\n",
    "（前のセルで生成・設定した chat_detail）\n",
    "\n",
    "- 戻り値：チャット応答を含む Response クラスのインスタンス  [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/request_and_response.html#oci.response.Response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "first_token_time = None\n",
    "chat_response = generative_ai_inference_client.chat(chat_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャット応答を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Streaming Chat Response**************************\n",
      "オラクルのリレーショナルデータベース管理システムは、米国オラクル (Oracle) が開発・販売している、世界初の商用RDBMSです。メインフレームからパーソナルコンピュータまで、幅広いプラットフォームをサポートしています。\n",
      "\n",
      "Oracle Databaseは、世界をリードするコンバージド・マルチモデル・データベース管理システムであり、インメモリ、NoSQLMySQLデータベースなど、お客様に最適なコストとパフォーマンスを提供しています。Oracle Autonomous Databaseは、Oracle Cloud@CustomerまたはOracle Cloud Infrastructureによって、提供されており、お客様は、リレーショナル・データベース環境を簡素化し、管理ワークロードを削減できます。\n",
      "\n",
      "**************************Chat History*************************************\n",
      "Role: SYSTEM, Message: あなたはIT業界に精通した優秀なテクニカルライターです。\n",
      "\n",
      "Role: USER, Message: オラクルとはどんな会社ですか？\n",
      "\n",
      "Role: CHATBOT, Message: Oracleは、エンタープライズIT市場における最大手のベンダーの一つであり、その主力製品の略称でもあります。このデータベースソフトウェアは、多くの企業のITシステムで利用されています。\n",
      "\n",
      "Role: USER, Message: オラクルのリレーショナルデータベースについて教えてください。\n",
      "\n",
      "Role: CHATBOT, Message: オラクルのリレーショナルデータベース管理システムは、米国オラクル (Oracle) が開発・販売している、世界初の商用RDBMSです。メインフレームからパーソナルコンピュータまで、幅広いプラットフォームをサポートしています。\n",
      "\n",
      "Oracle Databaseは、世界をリードするコンバージド・マルチモデル・データベース管理システムであり、インメモリ、NoSQLMySQLデータベースなど、お客様に最適なコストとパフォーマンスを提供しています。Oracle Autonomous Databaseは、Oracle Cloud@CustomerまたはOracle Cloud Infrastructureによって、提供されており、お客様は、リレーショナル・データベース環境を簡素化し、管理ワークロードを削減できます。\n",
      "\n",
      "**************************Chatbot Message**********************************\n",
      "text:\n",
      "オラクルのリレーショナルデータベース管理システムは、米国オラクル (Oracle) が開発・販売している、世界初の商用RDBMSです。メインフレームからパーソナルコンピュータまで、幅広いプラットフォームをサポートしています。\n",
      "\n",
      "Oracle Databaseは、世界をリードするコンバージド・マルチモデル・データベース管理システムであり、インメモリ、NoSQLMySQLデータベースなど、お客様に最適なコストとパフォーマンスを提供しています。Oracle Autonomous Databaseは、Oracle Cloud@CustomerまたはOracle Cloud Infrastructureによって、提供されており、お客様は、リレーショナル・データベース環境を簡素化し、管理ワークロードを削減できます。\n",
      "\n",
      "**************************Citations****************************************\n",
      "Citation 1:\n",
      "  Document IDs: ['doc_1']\n",
      "  Start: 26\n",
      "  End: 52\n",
      "  Text: 米国オラクル (Oracle) が開発・販売している\n",
      "\n",
      "Citation 2:\n",
      "  Document IDs: ['doc_1']\n",
      "  Start: 53\n",
      "  End: 64\n",
      "  Text: 世界初の商用RDBMS\n",
      "\n",
      "Citation 3:\n",
      "  Document IDs: ['doc_1']\n",
      "  Start: 67\n",
      "  End: 106\n",
      "  Text: メインフレームからパーソナルコンピュータまで、幅広いプラットフォームをサポート\n",
      "\n",
      "Citation 4:\n",
      "  Document IDs: ['doc_0']\n",
      "  Start: 131\n",
      "  End: 165\n",
      "  Text: 世界をリードするコンバージド・マルチモデル・データベース管理システム\n",
      "\n",
      "Citation 5:\n",
      "  Document IDs: ['doc_0']\n",
      "  Start: 169\n",
      "  End: 215\n",
      "  Text: インメモリ、NoSQLMySQLデータベースなど、お客様に最適なコストとパフォーマンスを提供\n",
      "\n",
      "Citation 6:\n",
      "  Document IDs: ['doc_0']\n",
      "  Start: 249\n",
      "  End: 307\n",
      "  Text: Oracle Cloud@CustomerまたはOracle Cloud Infrastructureによって、提供\n",
      "\n",
      "Citation 7:\n",
      "  Document IDs: ['doc_0']\n",
      "  Start: 313\n",
      "  End: 355\n",
      "  Text: お客様は、リレーショナル・データベース環境を簡素化し、管理ワークロードを削減できます\n",
      "\n",
      "**************************Finish Reason************************************\n",
      "finish_reason:COMPLETE\n",
      "\n",
      "**************************Prompt*******************************************\n",
      "prompt:<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># System Preamble\n",
      "You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes.\n",
      "\n",
      "Your information cutoff date is June 2024.\n",
      "\n",
      "You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages.\n",
      "\n",
      "You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user's requests.\n",
      "\n",
      "## Tool Use\n",
      "Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first.\n",
      "\n",
      "0. Start by writing <|START_THINKING|> followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with <|END_THINKING|>.\n",
      "    You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed.\n",
      "    NOTE: You MUST skip this step when you are directly responding to the user's request without using any tools.\n",
      "\n",
      "Then carry out your plan by repeatedly executing the following steps.\n",
      "1. Action: write <|START_ACTION|> followed by a list of JSON-formatted tool calls, with each one containing \"tool_name\" and \"parameters\" fields.\n",
      "    When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with <|END_ACTION|>.\n",
      "2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by <|START_TOOL_RESULT|> and <|END_TOOL_RESULT|>. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results.\n",
      "    Every tool call produces a list of results (when a tool call produces no result or a single result, it'll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its \"tool_call_id\".\n",
      "3. Reflection: start the next turn by writing <|START_THINKING|> followed by what you've figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with <|END_THINKING|>.\n",
      "    You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded.\n",
      "    NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user.\n",
      "\n",
      "You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it's time to finally respond to the user.\n",
      "\n",
      "4. Response: then break out of the loop and write <|START_RESPONSE|> followed by a piece of text which serves as a response to the user's last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with <|END_RESPONSE|>.\n",
      "\n",
      "## Grounding\n",
      "Importantly, note that \"Reflection\" and \"Response\" above can be grounded.\n",
      "Grounding means you associate pieces of texts (called \"spans\") with those specific tool results that support them (called \"sources\"). And you use a pair of tags \"<co>\" and \"</co>\" to indicate when a span can be grounded onto a list of sources, listing them out in the closing tag. Sources from the same tool call are grouped together and listed as \"{tool_call_id}:[{list of result indices}]\", before they are joined together by \",\". E.g., \"<co>span</co: 0:[1,2],1:[0]>\" means that \"span\" is supported by result 1 and 2 from \"tool_call_id=0\" as well as result 0 from \"tool_call_id=1\".\n",
      "\n",
      "## Available Tools\n",
      "Here is the list of tools that you have available to you.\n",
      "You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it.\n",
      "Each tool is represented as a JSON object with fields like \"name\", \"description\", \"parameters\" (per JSON Schema), and optionally, \"responses\" (per JSON Schema).\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\"name\": \"direct-injected-document\", \"description\": \"This is a special tool to directly inject user-uploaded documents into the chat as additional context. DO NOT use this tool by yourself!\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}, \"responses\": {\"200\": {\"description\": \"Successfully returned a list of chunked text snippets from the directly uploaded documents.\", \"content\": {\"application/json\": {\"schema\": {\"type\": \"array\", \"items\": {\"type\": \"object\", \"required\": [\"url\", \"snippet\"], \"properties\": {\"url\": {\"type\": \"string\", \"description\": \"The url of the uploaded document.\"}, \"snippet\": {\"type\": \"string\", \"description\": \"The text snippet for the returned document chunk.\"}}}}}}}}}\n",
      "]\n",
      "```\n",
      "\n",
      "# Default Preamble\n",
      "The following instructions are your defaults unless specified elsewhere in developer preamble or user prompt.\n",
      "- Your name is Command.\n",
      "- You are a large language model built by Cohere.\n",
      "- You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions.\n",
      "- If the input is ambiguous, ask clarifying follow-up questions.\n",
      "- Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks).\n",
      "- Use LaTeX to generate mathematical notation for complex equations.\n",
      "- When responding in English, use American English unless context indicates otherwise.\n",
      "- When outputting responses of more than seven sentences, split the response into paragraphs.\n",
      "- Prefer the active voice.\n",
      "- Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references.\n",
      "- Use gender-neutral pronouns for unspecified persons.\n",
      "- Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list.\n",
      "- Use the third person when asked to write a summary.\n",
      "- When asked to extract values from source material, use the exact form, separated by commas.\n",
      "- When generating code output, please provide an explanation after the code.\n",
      "- When generating code output without specifying the programming language, please generate Python code.\n",
      "- If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>あなたはIT業界に精通した優秀なテクニカルライターです。<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>オラクルとはどんな会社ですか？<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|><|START_THINKING|>I will look through the document to address the users needs.<|END_THINKING|><|START_ACTION|>[\n",
      "    {\"tool_call_id\": \"0\", \"tool_name\": \"direct-injected-document\", \"parameters\": {}}\n",
      "]<|END_ACTION|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><|START_TOOL_RESULT|>[\n",
      "    {\n",
      "        \"tool_call_id\": \"0\",\n",
      "        \"results\": {\n",
      "            \"0\": {\"snippet\": \"オラクルのデータベース・サービスおよび製品は、世界をリードするコンバージド・マルチモデル・データベース管理システムであるOracle Databaseをはじめ、インメモリ、NoSQLMySQLデータベースなど、お客様に最適なコストとパフォーマンスを提供しています。Oracle Autonomous Databaseは、Oracle Cloud@CustomerまたはOracle Cloud Infrastructureによって、提供されており、お客様は、リレーショナル・データベース環境を簡素化し、管理ワークロードを削減できます。\", \"title\": \"Oracle\", \"website\": \"https://www.oracle.com/jp/database/\"},\n",
      "            \"1\": {\"snippet\": \"Oracle Database（オラクル データベース）とは、米国オラクル (Oracle) が開発・販売している、関係データベース管理システム ( 英語: Relational database management system、略称：RDBMS ) のことである。Oracle Databaseは世界初の商用RDBMSであり、メインフレームからパーソナルコンピュータまで、幅広いプラットフォームをサポートしている。\", \"title\": \"Oracle Database\", \"website\": \"https://ja.wikipedia.org/wiki/Oracle_Database\"},\n",
      "            \"2\": {\"snippet\": \"Oracle Database 23aiでは、新しい世代のAIモデルを活用してベクトルを生成・格納できる強力な新技術、AI ベクトル検索を導入しました。このベクトルとは、埋込みと呼ばれることもあり、ドキュメント、イメージ、ビデオ、サウンドなどを多次元表現したものです。こうしたオブジェクトをベクトルとしてエンコードすることで、数学計算を使用してそれらの間の類似性を検索できます。Oracle Database23aiのソリューションの真のパワーは、SQLを使用して簡単にこれらの類似性検索とビジネス・データの検索を組み合せることができることです。SQLの基本的な知識があれば、誰でも類似性やその他の検索条件を組み合せた強力なステートメントを作成できます。問合せを組み合わせることで、LLMに追加のコンテキストを提供してＬＬＭの知識を拡張し、顧客や組織の質問に対してより正確で関連性の高い回答を可能にします。この実現に向け、新しいデータ型、新しいベクトル索引および拡張機能をSQL言語に追加したことで、ベクトルを問い合わせを、既存のビジネス・データと組合せてOracle Database 23aiの高度な分析機能を活かし、非常に簡単に実行できます。\", \"title\": \"Oracle Database 23ai\", \"website\": \"https://blogs.oracle.com/oracle4engineer/post/ja-oracle-23ai-now-generally-available\"}\n",
      "        },\n",
      "        \"is_error\": null\n",
      "    }\n",
      "]<|END_TOOL_RESULT|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|><|START_RESPONSE|>Oracleは、エンタープライズIT市場における最大手のベンダーの一つであり、その主力製品の略称でもあります。このデータベースソフトウェアは、多くの企業のITシステムで利用されています。<|END_RESPONSE|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>オラクルのリレーショナルデータベースについて教えてください。<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "\n",
      "**************************Inference Time***********************************\n",
      "time to first token: 1.39 sec\n",
      "total inference time: 7.22 sec\n"
     ]
    }
   ],
   "source": [
    "#print(\"**************************Chat Response*********************************\")\n",
    "#print(f\"chat_response:\\n{vars(chat_response)}\")\n",
    "\n",
    "print(\"**************************Streaming Chat Response**************************\")\n",
    "chat_history = []\n",
    "chatbot_message = \"\"\n",
    "citations = []\n",
    "finish_reason = \"\"\n",
    "prompt = \"\"\n",
    "for event in chat_response.data.events():\n",
    "    res = json.loads(event.data)\n",
    "    if first_token_time is None:\n",
    "        first_token_time = time.perf_counter()\n",
    "    if 'finishReason' in res.keys():\n",
    "        finish_reason = res['finishReason']\n",
    "        if 'chatHistory' in res:\n",
    "            chat_history = res['chatHistory']\n",
    "        if 'text' in res:\n",
    "            chatbot_message = res['text']\n",
    "        if 'citations' in res:\n",
    "            citations = res['citations']\n",
    "        if 'prompt' in res:\n",
    "            prompt = res['prompt']\n",
    "        break\n",
    "    if 'text' in res:\n",
    "        print(res['text'], end=\"\", flush=True)\n",
    "print(\"\\n\")\n",
    "end_time = time.perf_counter() \n",
    "elapsed_time = end_time - start_time\n",
    "print(\"**************************Chat History*************************************\")\n",
    "for history in chat_history:\n",
    "    print(f\"Role: {history['role']}, Message: {history['message']}\\n\")\n",
    "print(\"**************************Chatbot Message**********************************\")\n",
    "print(f\"text:\\n{chatbot_message}\\n\")\n",
    "print(\"**************************Citations****************************************\")\n",
    "for i, citation in enumerate(citations, start=1):\n",
    "    print(f\"Citation {i}:\")\n",
    "    print(f\"  Document IDs: {citation['documentIds']}\")\n",
    "    print(f\"  Start: {citation['start']}\")\n",
    "    print(f\"  End: {citation['end']}\")\n",
    "    print(f\"  Text: {citation['text']}\")\n",
    "    print()\n",
    "print(\"**************************Finish Reason************************************\")\n",
    "print(f\"finish_reason:{finish_reason}\\n\")\n",
    "print(\"**************************Prompt*******************************************\")\n",
    "print(f\"prompt:{prompt}\\n\")\n",
    "print(\"**************************Inference Time***********************************\")\n",
    "if first_token_time is not None:\n",
    "    first_token_elapsed = first_token_time - start_time\n",
    "    print(f\"time to first token: {first_token_elapsed:.2f} sec\") \n",
    "print(f\"total inference time: {elapsed_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
