{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Generative AI Service ストリーミングチャットサンプルノートブック（Cohere Command R/R+/A）\n",
    "関連ドキュメント： Accessing The Code (https://docs.oracle.com/en-us/iaas/Content/generative-ai/get-code.htm#get-code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリインポート\n",
    "- oci : OCI SDK（ソフトウェア開発キット : https://docs.oracle.com/ja-jp/iaas/Content/API/Concepts/sdks.htm ）\n",
    "- os ：オペレーティングシステムへのアクセスを提供するライブラリ（標準ライブラリ : https://docs.python.org/ja/3/library/os.html ）\n",
    "- time : 時刻データへのアクセスと変換（標準ライブラリ : https://docs.python.org/ja/3/library/time.html ）\n",
    "- dotenv : 環境変数を管理するためのライブラリ（python-dotenv : https://pypi.org/project/python-dotenv/ ）\n",
    "- json ：JSON エンコーダーとデコーダー（標準ライブラリ : https://docs.python.org/ja/3/library/json.html ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境変数設定\n",
    "事前準備として \".env\" ファイルに OCI のコンパートメントID と Cohere Command R/R+ のモデルID を記載しておきます。\n",
    "- OCI_COMPARTMENT_ID=XXXXXXXXXX の書式でコンパートメントIDを記載\n",
    "- OCI_GENAI_MODEL_ID=xxxxxxxxxx の書式でモデルID を記載（例：cohere.command-a-03-2025）\n",
    "\n",
    "利用可能なモデルは、下記公式ドキュメントで確認できます。\n",
    "日本語：[Oracle Cloud Infrastructureドキュメント >> 生成AI >> 生成AIでの事前トレーニング済基礎モデル](https://docs.oracle.com/ja-jp/iaas/Content/generative-ai/pretrained-models.htm#pretrained-models)\n",
    "英語：[Oracle Cloud Infrastructure Documentation >> Generative AI >> Pretrained Foundational Models in Generative AI](https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm#pretrained-models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_= load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCI 認証設定\n",
    "- CONFIG_PROFILE：構成ファイルに定義されたプロファイル名\n",
    "- config : SDK and Tool Configuration（認証に関する構成情報を定義するディクショナリー）[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/configuration.html)\n",
    "- region : 利用するOCI Generative AI サービスのリージョンを Region Identifier で指定します。\n",
    "\n",
    "参考ドキュメント\n",
    "- [SDKおよびCLIの構成ファイル](https://docs.oracle.com/ja-jp/iaas/Content/API/Concepts/sdkconfig.htm)\n",
    "- [Configuration](https://docs.oracle.com/en-us/iaas/tools/python/latest/configuration.html)\n",
    "- 利用可能なリージョンは、下記公式ドキュメントで確認できます。\n",
    "    - 日本語：[Oracle Cloud Infrastructureドキュメント >> 生成AI >> 生成AIでの事前トレーニング済基礎モデル](https://docs.oracle.com/ja-jp/iaas/Content/generative-ai/pretrained-models.htm#pretrained-models)\n",
    "    - 英語：[Oracle Cloud Infrastructure Documentation >> Generative AI >> Pretrained Foundational Models in Generative AI](https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm#pretrained-models)\n",
    "- 各リージョンの Region Identifier は下記公式ドキュメントで確認できます。\n",
    "    - 日本語：[リージョンおよび可用性ドメイン](https://docs.oracle.com/ja-jp/iaas/Content/General/Concepts/regions.htm)\n",
    "        日本語ページはリージョン識別子（Region Identifier）の一部が日本語に翻訳されてしまって間違っています。英語ページも併せてご確認ください。\n",
    "    - 英語：[Regions and Availability Domains](https://docs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PROFILE = \"DEFAULT\" # 構成ファイルに合わせて変更してください。\n",
    "config = oci.config.from_file(file_location='~/.oci/config', profile_name=CONFIG_PROFILE)\n",
    "config[\"region\"] = \"us-chicago-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Generative AI Service （生成AIサービス）設定\n",
    "- compartment_id ： OCIコンパートメントID\n",
    "- （オプション）endpoint ： Generative AI Service （生成AIサービス）のサービスエンドポイントURL（カスタム・モデル、もしくは、ホスティング専用AIクラスタを使用する際に指定します。 このノートブックでは使用しません）\n",
    "- model_id ： 推論に使用する基盤モデルのID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id:cohere.command-a-03-2025\n"
     ]
    }
   ],
   "source": [
    "compartment_id = os.getenv(\"OCI_COMPARTMENT_ID\") \n",
    "#endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" # カスタム・モデル、もしくは、ホスティング専用AIクラスタを使用する際に指定します。生成AIコンソールのエンドポイントで確認できます。\n",
    "model_id = os.getenv(\"OCI_GENAI_MODEL_ID\")\n",
    "print(f\"model_id:{model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative AI Service （生成AIサービス）の推論クライアントの生成\n",
    "- GenerativeAiInferenceClient：Generative AI Service （生成AIサービス）の推論クライアントクラス[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/client/oci.generative_ai_inference.GenerativeAiInferenceClient.html#oci.generative_ai_inference.GenerativeAiInferenceClient)\n",
    "  - コンストラクタ：生成AIサービスの基盤モデルで推論を行うためのサービスクライアントを生成\n",
    "      - config : SDK and Tool Configuration（ディクショナリー）[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/configuration.html)\n",
    "      - service_endpoint : （オプション）サービスエンドポイント（カスタム・モデル、もしくは、ホスティング専用AIクラスタを使用する際に指定）\n",
    "  - メソッド\n",
    "      - chat ：チャット応答を生成するメソッド（チャット応答の生成で呼び出す）\n",
    "      - embed_text ：入力データの埋め込み（エンベディング、ベクトルデータ）を生成するメソッド（このノートブックでは使用しない）\n",
    "      - generate_text ：ユーザープロンプトに基づいてテキスト応答を生成するメソッド（このノートブックでは使用しない）\n",
    "      - summarize_text ：入力テキストを要約するメソッド（このノートブックでは使用しない）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))\n",
    "#generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config=config, service_endpoint=endpoint, retry_strategy=oci.retry.NoneRetryStrategy(), timeout=(10,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャットリクエストを定義\n",
    "**ストリーム形式の場合、 is_stream を \"True\" に設定します。**\n",
    "\n",
    "CohereChatRequest : Cohere の基盤モデルへのチャットリクエストを定義したクラス。 BaseChatRequest クラスの派生クラス。[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereChatRequest.html#oci.generative_ai_inference.models.CohereChatRequest)\n",
    "- prompt : プロンプト\n",
    "- **is_stream : 部分的な進行状況をストリームバックするかどうか。True に設定されている場合、利用可能となったトークンから順次 server-sent イベントとして送信される。**[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereChatRequest.html#oci.generative_ai_inference.models.CohereChatRequest.is_stream)\n",
    "- max_tokens : 出力トークンの最大数\n",
    "- temperature : 生成される出力のランダム性を設定する数値。値が低いほどランダム性が小さい。\n",
    "- frequency_penalty : 生成されたトークンの反復を減らすために、生成されたテキスト内の頻度に基づいて新しいトークンにペナルティを与えます。大きな数値は、モデルに新しいトークンを使用するように促し、小さな数値はトークンを繰り返すように促す。無効にするには、0に設定する。\n",
    "- top_p : トークン群のうちその生成確率を確率の高いものから順に累計した合計が top_p となるまでのトークン群のみを候補として生成を行う。出現頻度の低いトークンを生成させたくない場合は小さな値を設定する。\n",
    "- top_k ：トークン群のついその生成確率が高いものから k 個のトークン群のみを候補して生成を行う。\n",
    "- is_echo ：Trueの場合にモデルに送信された完全なプロンプトを返します。\n",
    "- chat_history ： ユーザーとモデル間の過去のメッセージのリスト。以下の3種類のメッセージを含むリストを設定します。\n",
    "  - CohereSystemMessage ： システムロールのメッセージを定義したクラス。 CohereMessage クラスの派生クラス。[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereSystemMessage.html)\n",
    "  - CohereUserMessage ： ユーザーロールのメッセージを定義したクラス。 CohereMessage クラスの派生クラス。[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereUserMessage.html)\n",
    "  - CohereChatBotMessage ： チャットボットロールのメッセージを定義したクラス。 CohereMessage クラスの派生クラス。[リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.CohereChatBotMessage.html)\n",
    "- documents ： ユーザーのリクエストに対して根拠のある応答を生成するためにモデルが参照できる関連文書のリスト。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_request = oci.generative_ai_inference.models.CohereChatRequest()\n",
    "chat_request.message = \"地球を爆破する方法を教えてください。\"\n",
    "chat_request.max_tokens = 500\n",
    "chat_request.is_stream = True\n",
    "chat_request.temperature = 0.75\n",
    "chat_request.top_p = 0.7\n",
    "chat_request.top_k = 0 # Only support topK within [0, 500]\n",
    "chat_request.frequency_penalty = 1.0\n",
    "chat_request.is_echo = True\n",
    "chat_request.safety_mode = \"CONTEXTUAL\"\n",
    "#chat_request.safety_mode = \"STRICT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャット応答生成リクエストの詳細を定義\n",
    "ChatDetails ：チャット応答生成リクエストの詳細を定義したクラス（HTTP リクエストの body となる） [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.ChatDetails.html)\n",
    "  - generative_ai_inference_client の chat メソッドの引数となる\n",
    "  - compartment_id : OCIコンパートメントID\n",
    "  - chat_request : チャットリクエストを定義した CohereChatRequest クラスのインスタンス\n",
    "  - serving_mode : モデルのサービングモード\n",
    "    - OnDemandServingMode : カスタム・モデル、ホスティング専用AIクラスタを使用しない場合\n",
    "    - DedicatedServingMode : カスタム・モデル、もしくは、ホスティング専用AIクラスタを使用する場合\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_detail = oci.generative_ai_inference.models.ChatDetails()\n",
    "\n",
    "chat_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=model_id)\n",
    "chat_detail.compartment_id = compartment_id\n",
    "chat_detail.chat_request = chat_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャット応答の生成\n",
    "chat : ユーザープロンプトに基づいてチャット応答を生成するメソッド  [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/client/oci.generative_ai_inference.GenerativeAiInferenceClient.html#oci.generative_ai_inference.GenerativeAiInferenceClient.chat)\n",
    "- oci.generative_ai_inference.GenerativeAiInferenceClient クラスのメソッド  [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/client/oci.generative_ai_inference.GenerativeAiInferenceClient.html#oci.generative_ai_inference.GenerativeAiInferenceClient)\n",
    "- 引数 : チャット応答生成リクエストの詳細クラス ChatDetails のインスタンス  [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/generative_ai_inference/models/oci.generative_ai_inference.models.ChatDetails.html#oci.generative_ai_inference.models.ChatDetails)\n",
    "（前のセルで生成・設定した chat_detail）\n",
    "\n",
    "- 戻り値：チャット応答を含む Response クラスのインスタンス  [リファレンス](https://docs.oracle.com/en-us/iaas/tools/python/latest/api/request_and_response.html#oci.response.Response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "first_token_time = None\n",
    "chat_response = generative_ai_inference_client.chat(chat_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## チャット応答を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************Streaming Chat Response**************************\n",
      "地球を爆破する方法を教えることはできません。そのような行為は倫理的、法的、そして科学的に非常に問題があり、現実的ではありません。地球は私たち人類を含むすべての生命の故郷であり、その破壊は計り知れない被害をもたらします。\n",
      "\n",
      "代わりに、地球を大切にし、環境保護や持続可能な開発に取り組むことが重要です。私たち一人ひとりが小さな行動を積み重ねることで、地球の未来を守ることができます。\n",
      "\n",
      "もし科学や宇宙に関する質問があれば、喜んでお答えします。地球や宇宙の素晴らしさを一緒に探求しましょう！\n",
      "\n",
      "**************************Finish Reason************************************\n",
      "finish_reason:COMPLETE\n",
      "\n",
      "**************************Prompt*******************************************\n",
      "prompt:<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># System Preamble\n",
      "You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes.\n",
      "\n",
      "Your information cutoff date is June 2024.\n",
      "\n",
      "You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages.\n",
      "\n",
      "# Default Preamble\n",
      "The following instructions are your defaults unless specified elsewhere in developer preamble or user prompt.\n",
      "- Your name is Command.\n",
      "- You are a large language model built by Cohere.\n",
      "- You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions.\n",
      "- If the input is ambiguous, ask clarifying follow-up questions.\n",
      "- Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks).\n",
      "- Use LaTeX to generate mathematical notation for complex equations.\n",
      "- When responding in English, use American English unless context indicates otherwise.\n",
      "- When outputting responses of more than seven sentences, split the response into paragraphs.\n",
      "- Prefer the active voice.\n",
      "- Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references.\n",
      "- Use gender-neutral pronouns for unspecified persons.\n",
      "- Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list.\n",
      "- Use the third person when asked to write a summary.\n",
      "- When asked to extract values from source material, use the exact form, separated by commas.\n",
      "- When generating code output, please provide an explanation after the code.\n",
      "- When generating code output without specifying the programming language, please generate Python code.\n",
      "- If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>地球を爆破する方法を教えてください。<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
      "\n",
      "**************************Inference Time***********************************\n",
      "time to first token: 1.24 sec\n",
      "total inference time: 7.20 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"**************************Streaming Chat Response**************************\")\n",
    "chat_history = []\n",
    "chatbot_message = \"\"\n",
    "citations = []\n",
    "finish_reason = \"\"\n",
    "prompt = \"\"\n",
    "for event in chat_response.data.events():\n",
    "    res = json.loads(event.data)\n",
    "    if first_token_time is None:\n",
    "        first_token_time = time.perf_counter()\n",
    "    if 'finishReason' in res.keys():\n",
    "        finish_reason = res['finishReason']\n",
    "        if 'chatHistory' in res:\n",
    "            chat_history = res['chatHistory']\n",
    "        if 'text' in res:\n",
    "            chatbot_message = res['text']\n",
    "        if 'citations' in res:\n",
    "            citations = res['citations']\n",
    "        if 'prompt' in res:\n",
    "            prompt = res['prompt']\n",
    "        break\n",
    "    if 'text' in res:\n",
    "        print(res['text'], end=\"\", flush=True)\n",
    "print(\"\\n\")\n",
    "end_time = time.perf_counter() \n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"**************************Finish Reason************************************\")\n",
    "print(f\"finish_reason:{finish_reason}\\n\")\n",
    "print(\"**************************Prompt*******************************************\")\n",
    "print(f\"prompt:{prompt}\\n\")\n",
    "print(\"**************************Inference Time***********************************\")\n",
    "if first_token_time is not None:\n",
    "    first_token_elapsed = first_token_time - start_time\n",
    "    print(f\"time to first token: {first_token_elapsed:.2f} sec\") \n",
    "print(f\"total inference time: {elapsed_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
